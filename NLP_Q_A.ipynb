{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_Q_A.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/susmitmishra125/QA_NLP/blob/main/NLP_Q_A.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TlP2HPbXK3gj",
        "outputId": "888dd3c4-7367-4e24-f93e-086d442ee520"
      },
      "source": [
        "!git clone https://github.com/susmitmishra125/QA_NLP"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'QA_NLP'...\n",
            "remote: Enumerating objects: 63, done.\u001b[K\n",
            "remote: Counting objects: 100% (63/63), done.\u001b[K\n",
            "remote: Compressing objects: 100% (50/50), done.\u001b[K\n",
            "remote: Total 63 (delta 29), reused 22 (delta 8), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (63/63), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2FVm2TnyVV0M",
        "outputId": "156e009a-b415-45d3-d07b-b8a4df3b3450"
      },
      "source": [
        "!pip install contractions\n",
        "!pip install transformers"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting contractions\n",
            "  Downloading contractions-0.0.58-py2.py3-none-any.whl (8.0 kB)\n",
            "Collecting textsearch>=0.0.21\n",
            "  Downloading textsearch-0.0.21-py2.py3-none-any.whl (7.5 kB)\n",
            "Collecting pyahocorasick\n",
            "  Downloading pyahocorasick-1.4.2.tar.gz (321 kB)\n",
            "\u001b[K     |████████████████████████████████| 321 kB 5.3 MB/s \n",
            "\u001b[?25hCollecting anyascii\n",
            "  Downloading anyascii-0.3.0-py3-none-any.whl (284 kB)\n",
            "\u001b[K     |████████████████████████████████| 284 kB 35.8 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyahocorasick\n",
            "  Building wheel for pyahocorasick (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyahocorasick: filename=pyahocorasick-1.4.2-cp37-cp37m-linux_x86_64.whl size=85442 sha256=ea4ade00d3ba45ceb76596b0b13cc30f23e0ebf486dccdda66a3f85461611289\n",
            "  Stored in directory: /root/.cache/pip/wheels/25/19/a6/8f363d9939162782bb8439d886469756271abc01f76fbd790f\n",
            "Successfully built pyahocorasick\n",
            "Installing collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.0 contractions-0.0.58 pyahocorasick-1.4.2 textsearch-0.0.21\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.12.5-py3-none-any.whl (3.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1 MB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3 MB 35.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n",
            "Collecting sacremoses\n",
            "  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n",
            "\u001b[K     |████████████████████████████████| 895 kB 38.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 25.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.1.2-py3-none-any.whl (59 kB)\n",
            "\u001b[K     |████████████████████████████████| 59 kB 7.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n",
            "Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.1.2 pyyaml-6.0 sacremoses-0.0.46 tokenizers-0.10.3 transformers-4.12.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5-UdxUP1rTmP",
        "outputId": "e4b20054-7d93-4e4f-839a-9bd89f829de9"
      },
      "source": [
        "%cd QA_NLP"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/QA_NLP\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "25hyciYQrcU0",
        "outputId": "a25d49df-ded7-477e-9b3c-7eb612950d3e"
      },
      "source": [
        "# Importing modules\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from transformers import BertTokenizer\n",
        "from transformers import BertForQuestionAnswering\n",
        "import torch\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "import contractions\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "from collections import defaultdict\n",
        "import nltk\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "nltk.download('punkt')  # For tokenizers\n",
        "nltk.download('stopwords')\n",
        "\n",
        "\n",
        "# importing dataset\n",
        "data = open('passage.txt', 'r', encoding=\"utf-8\").read()\n",
        "questions = open('question_list.txt', 'r').readlines()\n",
        "\n",
        "# preprocess for modern method\n",
        "\n",
        "\n",
        "def preprocessing(rawReadCorpus, complete_preprocess=False):\n",
        "    pattern = \"^a-zA-Z0-9_\"\n",
        "    rawReadCorpus = contractions.fix(rawReadCorpus)\n",
        "    text_sent = sent_tokenize(rawReadCorpus)  # to split the sentences\n",
        "    text_sent = [sent.lower() for sent in text_sent]  # to convert to lowercase\n",
        "    text_sent = [\"\".join([char for char in text if char not in string.punctuation])\n",
        "                 for text in text_sent]  # removed punctuation\n",
        "    text_sent = [word_tokenize(sent) for sent in text_sent]\n",
        "    # text_sent = [\"\".join([char for char in text if char not in ]) for text in text_sent] # removed punctuation\n",
        "    for i in range(len(text_sent)):\n",
        "        sent = \" \".join(text_sent[i])\n",
        "        sent = re.sub(pattern, ' ', sent)\n",
        "        sent = sent.replace(\"“\", \" \")\n",
        "        sent = sent.replace(\"”\", \" \")\n",
        "        sent = sent.replace(\"—\", \" \")\n",
        "        sent = sent.replace(\"_\", \" \")\n",
        "        text_sent[i] = sent.split(' ')\n",
        "        text_sent[i] = [i for i in text_sent[i] if i != '']\n",
        "        # print(text_sent[i])\n",
        "    if complete_preprocess:\n",
        "        ps = nltk.porter.PorterStemmer()\n",
        "        for i in range(len(text_sent)):\n",
        "            text_sent[i] = [ps.stem(j) for j in text_sent[i]]\n",
        "    i = len(text_sent)-1\n",
        "    while(i >= 0):\n",
        "        if 'chapter' in text_sent[i]:\n",
        "            # print(text_sent[i])\n",
        "            del(text_sent[i])\n",
        "        i -= 1\n",
        "    return text_sent\n",
        "\n",
        "\n",
        "def answer_question(question, answer_text, model, tokenizer):\n",
        "    '''\n",
        "    Takes a question string and an answer_text string (which contains the\n",
        "    answer), and identifies the words within the answer_text that are the\n",
        "    answer. Prints them out.\n",
        "    '''\n",
        "    input_ids = tokenizer.encode(question, answer_text)\n",
        ".\n",
        "    sep_index = input_ids.index(tokenizer.sep_token_id)\n",
        "    num_seg_a = sep_index + 1\n",
        "    num_seg_b = len(input_ids) - num_seg_a\n",
        "    segment_ids = [0]*num_seg_a + [1]*num_seg_b\n",
        "    assert len(segment_ids) == len(input_ids)\n",
        "    outputs = model(torch.tensor([input_ids]),  # The tokens representing our input text.\n",
        "                    # The segment IDs to differentiate question from answer_text\n",
        "                    token_type_ids=torch.tensor([segment_ids]),\n",
        "                    return_dict=True)\n",
        "    start_scores = outputs.start_logits\n",
        "    end_scores = outputs.end_logits\n",
        "    answer_start = torch.argmax(start_scores)\n",
        "    answer_end = torch.argmax(end_scores)\n",
        "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "    answer = tokens[answer_start]\n",
        "    for i in range(answer_start + 1, answer_end + 1):\n",
        "        if tokens[i][0:2] == '##':\n",
        "            answer += tokens[i][2:]\n",
        "        else:\n",
        "            answer += ' ' + tokens[i]\n",
        "    return (answer, start_scores[0][answer_start], end_scores[0][answer_end])\n",
        "\n",
        "\n",
        "def query(question, text_sent, model, tokenizer, n_sent=3):\n",
        "    outputs = []\n",
        "    sent_count = len(text_sent)\n",
        "    ans = 0\n",
        "    ans_text = 'NA'\n",
        "    for i in tqdm(range(sent_count-n_sent+1)):\n",
        "        segment = text_sent[i:i+n_sent]\n",
        "        passage = ''\n",
        "        for sent in segment:\n",
        "            passage += ' '.join(sent)+'. '\n",
        "        output = answer_question(question, passage, model, tokenizer)\n",
        "\n",
        "        outputs.append((output[0], 2*(output[1]*output[2])/(output[1]+output[2])))\n",
        "        if(len(outputs)>10):\n",
        "            outputs.sort(key=lambda x: x[1], reverse=True)\n",
        "            outputs=outputs[:10]\n",
        "\n",
        "    cnt = Counter()\n",
        "    for output in outputs:\n",
        "        cnt[output[0]] += 1\n",
        "    return cnt.most_common()[0][0]\n",
        "\n",
        "\n",
        "def modern():\n",
        "    text_sent = preprocessing(data)\n",
        "    model = BertForQuestionAnswering.from_pretrained(\n",
        "        'bert-large-uncased-whole-word-masking-finetuned-squad')\n",
        "    tokenizer = BertTokenizer.from_pretrained(\n",
        "        'bert-large-uncased-whole-word-masking-finetuned-squad')\n",
        "    for i in range(len(questions)):\n",
        "        # question in questions:\n",
        "        questions[i] = questions[i].replace('\\n', '')\n",
        "        print(questions[i])\n",
        "        ans = query(questions[i], text_sent=text_sent,model = model,tokenizer = tokenizer)\n",
        "        print(ans)\n",
        "\n",
        "from numpy import dot\n",
        "from numpy.linalg import norm\n",
        "\n",
        "# cos_sim = dot(a, b)/(norm(a)*norm(b))\n",
        "def traditional():\n",
        "    print(\"starting preprocessing\")\n",
        "    text_sent = preprocessing(data,complete_preprocess=False)\n",
        "    text_sent = [' '.join(sent) for sent in text_sent]\n",
        "    print(\"complete preprocessing\")\n",
        "    vectorizer = CountVectorizer(ngram_range=(1, 3))\n",
        "    \n",
        "    sentence_vectors = vectorizer.fit_transform(text_sent).toarray()\n",
        "\n",
        "    for question in questions:\n",
        "        ans = \"NA\"\n",
        "        max_cos = 0\n",
        "        q_vec = vectorizer.transform([question]).toarray()[0]\n",
        "        for i in range(len(text_sent)):\n",
        "            ans_vec = sentence_vectors[i]\n",
        "            cos_sim = dot(ans_vec,q_vec)/(norm(ans_vec)*norm(q_vec)+1e-9)\n",
        "            if cos_sim>max_cos:\n",
        "                # print(sum(q_vec),sum(ans_vec))\n",
        "                ans = text_sent[i]\n",
        "                max_cos = cos_sim\n",
        "        print(question)\n",
        "        print(ans,max_cos,'\\n')\n",
        "    # print(sentence_vectors.toarr)\n",
        "# traditional()\n",
        "modern()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "what is the colour of eyes of the white rabbit?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 289/289 [07:09<00:00,  1.49s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "white\n",
            "What is the label on the jar in the shelf?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 289/289 [06:51<00:00,  1.42s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "orange marmalade\n",
            "What was on the solid glass three legged table?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 289/289 [06:25<00:00,  1.33s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a tiny golden key\n",
            "Who was splendidly dressed and carrying a pair of white kid gloves in one hand and a large fan in the other?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 289/289 [07:03<00:00,  1.47s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "the white rabbit\n",
            "Who was afraid of being drowned in own tears?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 289/289 [06:18<00:00,  1.31s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "alice\n",
            "What did Alice pull out of her pocket as prizes?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 289/289 [06:21<00:00,  1.32s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a box of comfits\n",
            "What was written on the door of bright brass plate?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 289/289 [06:35<00:00,  1.37s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "w rabbit\n",
            "Whom did Alice gave one sharp kick?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 289/289 [06:34<00:00,  1.37s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bill\n",
            "What happened when Alice swallowed one of the cakes?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 289/289 [06:42<00:00,  1.39s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "she began shrinking directly\n",
            "What was the large blue caterpillar doing?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 289/289 [06:48<00:00,  1.41s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "digging for apples\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vi8-JvFjzuP8"
      },
      "source": [
        "# This function runs the modern method on question present in passage.txt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2YDYC-Yyyx7C"
      },
      "source": [
        "modern()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLp3KGYt0APR"
      },
      "source": [
        "# This function runs the traditional method on questions present in passage.txt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LIJf7r3y_Zt"
      },
      "source": [
        "traditional()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yf0W7RG40M3a"
      },
      "source": [
        "# For Testing new questions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWVeh7PbrcSy"
      },
      "source": [
        "def answerquery():\n",
        "    question = input(\"enter the question\")\n",
        "    \n",
        "\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xW8Q69lzrb8p"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}